\section{Related Work}
\label{sec:related}

\subsection{Large Language Models}

Large Language Models (LLMs) have demonstrated remarkable capabilities and gained surging popularity in recent years, ever since the release of ChatGPT~\citep{OpenAI2023ChatGPT}. Over the years, open-source models like Llama~\citep{grattafiori2024llama3herdmodels}, Phi~\citep{abdin2024phi4technicalreport,abdin2024phi3technicalreporthighly}, and Qwen~\citep{yang2024qwen2technicalreport,yang2025qwen3technicalreport} developed by companies like Meta, Microsoft, and Alibaba further facilitated a widespread integration of AI into diverse workflows and everyday applications. Leveraging their large parameter sizes and extensive post-training, LLMs are capable of performing a diverse array of Natural Language Processing (NLP) tasks~\citep{wan2023gptreincontextlearningrelation}. One of the key aspects of the generative capabilities of these models is their ability to generate structured data and transform data from one type to another while maintaining strict adherence to specified formats~\citep{guo2024large}. In this paper, we design a new and comprehensive benchmark that evaluates the capability of LLMs to understand, generate, and manipulate structured data across a range of complex, real-world tasks.


\subsection{Evaluation of LLMs}

Evaluating structured output has become a focal point for understanding LLM's limitations~\citep{ning2025picopeerreviewllms}. SoEval~\citep{liu2024soeval} offers a fast, rule-based check for JSON and XML, but its flat schemas fail to reveal errors in deeper hierarchies. StrucText-Eval~\citep{gu2024structext} shifts the task to reasoning over structure-rich text (JSON, YAML, LaTeX) rather than generating the structures themselves, while FOFO~\citep{xia2024fofo} extends to domains such as law and finance yet covers only a few formats and still relies on human verification. Developer-focused suites like StackEval~\citep{shah2024stackeval} for HTML, CSS, and plotting libraries, and CodeXGLUE~\citep{lu2021codexglue} for multilingual code tasks remain limited to programming artifacts, and Struc-Bench~\citep{tang-etal-2024-struc} concentrates on tabular generation with bespoke metrics. Each benchmark highlights a part of the challenge—be it format adherence, domain coverage, or table fidelity. However, none simultaneously demands broad format coverage, automated grading, and robust transformation capabilities. StructEval addresses these gaps by spanning 18 code and non-code formats, unifying generation, completion, and conversion tasks, and scoring outputs with fully automated structural and vision-based metrics, offering a comprehensive lens on how well LLMs respect and manipulate complex schemas.


\subsection{Structured Output Generation}
The ability to generate structured outputs is central to many real-world applications of LLMs~\citep{gu2024structext,tang-etal-2024-struc}. These outputs are not only expected to be semantically coherent but must also adhere strictly to syntactic and structural constraints—violations of which can lead to parsing failures, rendering errors, or broken downstream applications. Common tasks include generating JSON for API responses~\citep{Geng2025JSONSchemaBenchAR}, YAML or TOML for configuration files~\citep{Peddireddy2024EffectiveWA}, HTML or React for UI components~\citep{Si2024Design2CodeBM}, and LaTeX or Markdown for technical writing~\citep{Wen2024OverleafCopilotEA}. Moreover, in data science, models are used to transform unstructured descriptions into structured formats like CSV or tables for integration into analysis pipelines~\citep{Li2023TableGPTTG,Su2024TableGPT2AL}. In publishing and education, tools that convert textual prompts into diagrams (e.g., using TikZ, SVG, or Mermaid) help automate visualization generation~\citep{Lee2025FromTT,Rodriguez2023StarVectorGS,Ku2025TheoremExplainAgentTM}. Despite its significance, structured output generation remains challenging due to the need for models to internalize both syntax rules and hierarchical schema relationships across a wide variety of formats. Our \structeval first conducts a comprehensive evaluation of existing LLMs on both renderable and non-renderable tasks, showing that they still struggle to correctly generate some data formats including TOML, SVG, and Mermaid. 
