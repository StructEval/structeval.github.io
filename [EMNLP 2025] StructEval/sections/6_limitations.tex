\section*{Limitations}
\label{sec:limitations}
\textbf{Non-interactive formats} Our benchmark focuses on evaluating LLMs’ ability to generate static visual rendering formats such as HTML, React, Mermaid, etc. While this approach effectively assesses the model’s capacity to produce well-structured and visually coherent outputs, it is currently limited to single-page, non-interactive formats. The evaluation does not account for dynamic behaviors such as button interactions, page transitions, animations, or scroll events, which are essential to many real world user interfaces. Future work could extend the benchmark to include dynamic rendering tasks, enabling a more comprehensive assessment of LLM capabilities in producing fully interactive and responsive user experiences.

\textbf{Expert Review} While our dataset underwent a two-pass expert review process to ensure correctness, diversity, and minimize potential biases, the initial content was still generated by large language models. Despite expert oversight, residual biases inherent in the model outputs may persist, particularly in subtle or context-dependent scenarios that are challenging to detect through manual review. Moreover, expert validation, while thorough, may not fully capture the wide range of cultural, social, or contextual sensitivities relevant to diverse user populations. Future work could incorporate broader multi-annotator audits or automated bias detection techniques to further enhance dataset reliability and inclusiveness.
