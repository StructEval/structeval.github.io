
\section{Example Appendix}
\label{sec:appendix}

\subsection{VQA Prompt Template}
\label{app:vqa-prompt}

The following prompt is used for all renderable tasks evaluated using the vision-language model (VLM). The prompt is designed to verify whether visual requirements are satisfied based solely on the rendered output image.

\begin{quote}
\texttt{You are given an image and a list of question-answer pairs. \\
For each pair, verify if the image content supports the expected answer based on the corresponding question. \\
Base your judgment solely on the visual content of the provided image, and the question. \\
Do not use any external information or common-sense reasoning beyond what is visible. \\
Respond with a JSON object mapping each question number to true or false (e.g., \{"1": true, "2": false\}). \\
If the image is unclear or does not contain enough information to answer, use \texttt{null} for that question. \\
Here are the question-answer pairs: \{qa\_list\}}
\end{quote}


\subsection{Evaluation for Non-Renderable Formats}
\label{sec:nonrenderable-eval}

We evaluate the correctness of structured outputs in non-renderable formats such as JSON, XML, YAML, CSV, and TOML using a rule-based path-checking mechanism. Each task defines a set of \textit{dot-path rules}, which specify keys, nested structures, and list positions that must be present in the model-generated output.

The evaluation process saves the generated output to a file (based on the format), and checks the presence of required paths using a custom checker that supports robust path logic, including nesting, indexing, wildcards, backtick quoting, and format-specific constructs (e.g., CSV headers, XML attributes).

\paragraph{Path Rule Derivation.}
The way dot-path rules are created differs depending on the task type:

\textbf{Non-Renderable Generation Tasks} Each task prompt includes feature requirements stated in natural language. These requirements define target keys and their relationships to one another (e.g., nesting depth, list membership). Annotators translate each requirement into a concrete dot-path rule using the syntax rules shown in Table~\ref{tab:path-checker}.

\textbf{Non-Renderable Conversion Tasks} In these tasks, the input is itself a structured format (e.g., YAML or XML). We use an LLM to parse the structural schema of the input—identifying key names, nesting levels, and list structures—and convert them into target dot-path rules that the generated output must preserve.


\paragraph{Supported Rule Types.} Our evaluation supports a variety of path formats as shown in Table~\ref{tab:path_checker}.

\input{tables/path_checker}

This approach ensures that models are not only producing syntactically valid outputs, but also preserving the expected structural relationships. 


\section{Dataset Design}
\label{sec:appendix-prompts}

Each benchmark task in StructEval is paired with a prompt used to query the model. These prompts are tailored to the task type and follow consistent structural templates. Below we describe the prompt formats used for each of the three categories: generation of renderable formats, generation of non-renderable formats, and format-to-format conversion tasks.

\subsection{Generation: Renderable Formats}

Renderable generation tasks require the model to produce a structured output (e.g., HTML, LaTeX, React, Angular) that can be visually rendered. The prompt includes:

\begin{itemize}
    \item A system instruction indicating the target format.
    \item A high-level task description.
    \item A set of feature requirements describing visual, spatial, or structural expectations using precise and objective language.
\end{itemize}

\paragraph{Example Prompt (Renderable Generation):}
\begin{lstlisting}[breaklines=true, basicstyle=\ttfamily\small]

Please output HTML code.

Task:
Design a webpage that presents a user's travel itinerary.

Feature Requirements:
- Include a centered <h1> header with the text "Trip Summary".
- Use a <table> to list destinations; include 3 rows and 2 columns.
- Apply a class "highlight" to the second row.
- Add a <button> labeled "Export PDF" at the bottom of the page.
\end{lstlisting}

\paragraph{Associated VQA Metrics:}
\begin{itemize}
    \item Q: What text is displayed in the <h1> header? \quad A: \texttt{Trip Summary}
    \item Q: How many rows are in the table? \quad A: \texttt{3}
    \item Q: What class is applied to the second table row? \quad A: \texttt{highlight}
    \item Q: What text is on the button at the bottom? \quad A: \texttt{Export PDF}
\end{itemize}

\subsection{Generation: Non-Renderable Formats}

For structured formats that are not renderable (e.g., JSON, YAML, XML), prompts follow a similar structure:

\begin{itemize}
    \item A system instruction indicating the target format.
    \item A high-level task description.
    \item A list of field-level feature requirements, expressed in natural language, specifying keys, types, nesting, and relationships.
\end{itemize}

\paragraph{Example Prompt (Non-Renderable Generation):}
\begin{lstlisting}[breaklines=true, basicstyle=\ttfamily\small]
Please output JSON code.

Task:
Summarize metadata about a fictional scientific article.

Feature Requirements:
1. The top-level field "title" holds the article title as a string.
2. The field "authors" is a list containing exactly 2 items.
3. Each item in "authors" contains "name" (string) and "affiliation" (string).
4. The "publication.year" field gives the year as an integer.
5. The "keywords" field is a list of strings.
\end{lstlisting}

\paragraph{Associated Key Validation Paths:}
\begin{itemize}
    \item \texttt{title}
    \item \texttt{authors[0].name}
    \item \texttt{authors[1].affiliation}
    \item \texttt{publication.year}
    \item \texttt{keywords[2]}
\end{itemize}

\subsection{Format-to-Format Conversion Tasks}

In conversion tasks, the model is asked to transform structured code from one format to another. The prompt includes:

\begin{itemize}
    \item A single instruction indicating the source and target formats.
    \item A code block wrapped in \texttt{<code>} tags to denote the input content.
\end{itemize}

\paragraph{Example Prompt (Conversion):}
\begin{lstlisting}[breaklines=true, basicstyle=\ttfamily\small]
Please convert the following YAML into JSON.

<code>
title: "Ocean Life Report"
year: 2022
species:
  - name: "Blue Whale"
    endangered: true
  - name: "Clownfish"
    endangered: false
</code>
\end{lstlisting}

\paragraph{Extracted Structure for Validation:}
\begin{itemize}
    \item \texttt{title}
    \item \texttt{year}
    \item \texttt{species[0].name}
    \item \texttt{species[0].endangered}
    \item \texttt{species[1].name}
\end{itemize}


\section{Evaluation Metrics}
\label{sec:evaluation-metrics}

We evaluate structured output quality using format-aware metrics tailored to the output type. For renderable formats (e.g., HTML, Matplotlib), we apply a visual evaluation pipeline based on VQA; for non-renderable formats (e.g., JSON, YAML, TOML), we use structural path-based checks.

\subsection{Visual Question Answering (VQA) for Renderable Formats}

To assess whether visual outputs match the user-specified requirements, we render the generated content and apply a vision-language model to answer task-specific questions. Each example includes 5--10 VQA pairs designed to capture visual layout, content, and structure.

\paragraph{Prompt Example (Renderable Generation):}
\begin{lstlisting}[breaklines=true, basicstyle=\ttfamily\small]
Please output Matplotlib.

Task:
Generate a scatter plot showing altitude vs. air pressure.

Feature Requirements:
- Plot 7 data points.
- X-axis label: "Altitude (m)", Y-axis: "Pressure (hPa)".
- Use red circle markers.
- Title: "Pressure vs Altitude".
- Axis ranges: x 1000 to 5000, y 900 to 1050.
- Show x-axis grid lines.
- Add legend labeled "Measurement Data" in lower right.
\end{lstlisting}

\paragraph{VQA Metrics:}
\begin{itemize}
    \item Q: What is the title of the plot? \quad A: \texttt{Pressure vs Altitude}
    \item Q: How many data points are plotted? \quad A: \texttt{7}
    \item Q: What marker style and color are used? \quad A: \texttt{Red circle markers}
    \item Q: What is the x-axis label? \quad A: \texttt{Altitude (m)}
    \item Q: Where is the legend located? \quad A: \texttt{Lower right}
\end{itemize}

This method enables semantic verification of visual properties not easily captured by text matching.

\subsection{Key Validation Metrics for Non-Renderable Formats}

For non-renderable outputs, we define a list of required keys and structures based on the English feature requirements. These are converted into dot-path expressions and checked using a rule-based structural parser.

\paragraph{Prompt Example (Non-Renderable Generation):}
\begin{lstlisting}[breaklines=true, basicstyle=\ttfamily\small]
Please output TOML code.

Task:
Create a TOML file describing a fictional museum.

Feature Requirements:
1. The museum has a name field at the top level representing its official title as a string.
2. Inside the museum, there is a location object that includes both a city and a country field, each as strings.
3. The museum also contains a director object, which holds the director's full name as a string.
4. The museum has a list of galleries. The first gallery includes a title (string) and a floor number (integer).
5. Each gallery contains a list of artworks. The first artwork has a title and an artist name, both as strings.
6. The artwork also specifies its year of creation as an integer, and includes dimensions as a nested object.
7. Within the dimensions, there are fields for height and width in centimeters, both as numeric values.
8. A boolean field indicates whether the artwork is currently on display.
\end{lstlisting}

% \paragraph{Raw Output Metric Paths:}
% \begin{itemize}
%     \item \texttt{museum.name}
%     \item \texttt{museum.location.city}
%     \item \texttt{museum.director.full\_name}
%     \item \texttt{museum.galleries[0].artworks[0].title}
%     \item \texttt{museum.galleries[0].artworks[0].dimensions.height\_cm}
% \end{itemize}

\subsection{Scoring}



The output is parsed and validated against these rules using a path-checking engine that supports nested structures, indexed lists, and attributes across JSON, YAML, TOML, XML, and CSV.

\subsection{Scoring}






\subsection{Task Distributions}
\input{tables/task_dist}
